{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db1SzVnG8Ieq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the neural network architecture\n",
        "class PINN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(PINN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(20, activation='sigmoid')\n",
        "        self.dense2 = tf.keras.layers.Dense(20, activation='sigmoid')\n",
        "        self.dense2 = tf.keras.layers.Dense(20, activation='tanh')\n",
        "        self.dense2 = tf.keras.layers.Dense(20, activation='relu')\n",
        "        self.out = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.out(x)\n",
        "\n",
        "# Initialize the model\n",
        "model = PINN()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "x_data = np.linspace(0, 0.2, 11).reshape(-1, 1).astype(np.float32)\n",
        "T_data = 10 + 100*x_data - 500*x_data**2  # Example quadratic temperature distribution\n",
        "print(x_data)\n",
        "print(T_data)\n",
        "# Convert data to TensorFlow tensors\n",
        "x_data_tensor = tf.convert_to_tensor(x_data, dtype=tf.float32)\n",
        "T_data_tensor = tf.convert_to_tensor(T_data, dtype=tf.float32)\n",
        "print(x_data_tensor)\n",
        "print(T_data_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf4vLx6H8JJs",
        "outputId": "fa39a560-689b-4f87-e532-efe7f4f29043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  ]\n",
            " [0.02]\n",
            " [0.04]\n",
            " [0.06]\n",
            " [0.08]\n",
            " [0.1 ]\n",
            " [0.12]\n",
            " [0.14]\n",
            " [0.16]\n",
            " [0.18]\n",
            " [0.2 ]]\n",
            "[[10.      ]\n",
            " [11.8     ]\n",
            " [13.2     ]\n",
            " [14.2     ]\n",
            " [14.8     ]\n",
            " [15.      ]\n",
            " [14.8     ]\n",
            " [14.2     ]\n",
            " [13.200001]\n",
            " [11.799999]\n",
            " [ 9.999998]]\n",
            "tf.Tensor(\n",
            "[[0.  ]\n",
            " [0.02]\n",
            " [0.04]\n",
            " [0.06]\n",
            " [0.08]\n",
            " [0.1 ]\n",
            " [0.12]\n",
            " [0.14]\n",
            " [0.16]\n",
            " [0.18]\n",
            " [0.2 ]], shape=(11, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[10.      ]\n",
            " [11.8     ]\n",
            " [13.2     ]\n",
            " [14.2     ]\n",
            " [14.8     ]\n",
            " [15.      ]\n",
            " [14.8     ]\n",
            " [14.2     ]\n",
            " [13.200001]\n",
            " [11.799999]\n",
            " [ 9.999998]], shape=(11, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_physics_loss(model, x):\n",
        "    with tf.GradientTape() as tape2:\n",
        "        tape2.watch(x)\n",
        "        with tf.GradientTape() as tape1:\n",
        "            tape1.watch(x)\n",
        "            T_pred = model(x)\n",
        "        dT_dx = tape1.gradient(T_pred, x)\n",
        "    d2T_dx2 = tape2.gradient(dT_dx, x)\n",
        "    return tf.reduce_mean(tf.square(d2T_dx2))"
      ],
      "metadata": {
        "id": "8v0Tm67Q8LBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Custom training loop\n",
        "for epoch in range(1000):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        T_pred = model(x_data_tensor)\n",
        "        data_loss = tf.reduce_mean(tf.square(T_pred - T_data_tensor))\n",
        "        physics_loss = compute_physics_loss(model, x_data_tensor)\n",
        "        total_loss = data_loss + physics_loss\n",
        "\n",
        "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss.numpy()}, Data Loss: {data_loss.numpy()}, Physics Loss: {physics_loss.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJzJNzOq8Nqj",
        "outputId": "d5e5df52-be4e-4faf-c747-8f7204079ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 182.6131591796875, Data Loss: 182.6131591796875, Physics Loss: 1.4385896918156504e-07\n",
            "Epoch 100, Loss: 79.13604736328125, Data Loss: 79.13591003417969, Physics Loss: 0.00013764764298684895\n",
            "Epoch 200, Loss: 14.509834289550781, Data Loss: 14.505776405334473, Physics Loss: 0.004057638347148895\n",
            "Epoch 300, Loss: 3.298125982284546, Data Loss: 3.2855887413024902, Physics Loss: 0.012537333182990551\n",
            "Epoch 400, Loss: 3.1367242336273193, Data Loss: 3.1234781742095947, Physics Loss: 0.013246121816337109\n",
            "Epoch 500, Loss: 3.1351890563964844, Data Loss: 3.123079538345337, Physics Loss: 0.012109589762985706\n",
            "Epoch 600, Loss: 3.133820056915283, Data Loss: 3.1228716373443604, Physics Loss: 0.010948444716632366\n",
            "Epoch 700, Loss: 3.1325089931488037, Data Loss: 3.1226625442504883, Physics Loss: 0.009846384637057781\n",
            "Epoch 800, Loss: 3.131274938583374, Data Loss: 3.122454881668091, Physics Loss: 0.00882002990692854\n",
            "Epoch 900, Loss: 3.1301276683807373, Data Loss: 3.1222503185272217, Physics Loss: 0.007877398282289505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict temperatures using the trained model\n",
        "T_pred = model(x_data_tensor)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x_data, T_data, label='Actual Data')\n",
        "plt.plot(x_data, T_pred.numpy(), label='PINN Prediction', color='red')\n",
        "plt.xlabel('Position (m)')\n",
        "plt.ylabel('Temperature (C)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "VnjWQdIx8P72",
        "outputId": "5d83c532-272c-4158-901d-d77dcab4e7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the neural network\n",
        "class HeatConductionNN(nn.Module):\n",
        "    def __init__(self, num_neurons=20, num_layers=3):\n",
        "        super().__init__()\n",
        "        layers = [nn.Linear(1, num_neurons), nn.Tanh()]\n",
        "        for _ in range(num_layers-1):\n",
        "            layers += [nn.Linear(num_neurons, num_neurons), nn.Tanh()]\n",
        "        layers.append(nn.Linear(num_neurons, 1))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = HeatConductionNN()\n",
        "\n",
        "# Define the physics-informed loss\n",
        "def physics_informed_loss(model, x, T_actual):\n",
        "    # Use automatic differentiation to compute the second derivative\n",
        "    x.requires_grad = True\n",
        "    T_pred = model(x)\n",
        "    dT_dx = torch.autograd.grad(T_pred, x, grad_outputs=torch.ones_like(T_pred), create_graph=True)[0]\n",
        "    d2T_dx2 = torch.autograd.grad(dT_dx, x, grad_outputs=torch.ones_like(dT_dx), create_graph=True)[0]\n",
        "\n",
        "    # Loss for the differential equation\n",
        "    pde_loss = torch.mean(d2T_dx2**2)\n",
        "\n",
        "    # Loss for the data points\n",
        "    data_loss = torch.mean((T_pred - T_actual)**2)\n",
        "\n",
        "    return pde_loss + data_loss\n",
        "\n",
        "# Training loop\n",
        "T_data = [376.183183,\t376.079104,\t375.962007,\t375.814418,\t375.707068,\t375.600435,\t375.521479,\t375.386663,\t375.208900,\t375.126933,\t374.882827]\n",
        "\n",
        "# Convert the actual temperature data to a PyTorch tensor\n",
        "T_data_tensor = torch.tensor(T_data, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Training loop\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
        "x_data = torch.linspace(0, 0.2, 11).reshape(-1, 1)\n",
        "tolerance = 1e-1  # Set a threshold for early stopping\n",
        "previous_loss = float('inf')  # Initialize previous loss to a large value\n",
        "patience = 50  # Set how many epochs to wait after last time the monitored quantity improved\n",
        "patience_counter = 0  # Counter for patience\n",
        "\n",
        "for epoch in range(100000):\n",
        "    optimizer.zero_grad()\n",
        "    loss = physics_informed_loss(model, x_data, T_data_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_value = loss.item()\n",
        "    print(f'Epoch {epoch}, Loss: {loss_value}')\n",
        "\n",
        "    # Check for early stopping condition\n",
        "    if abs(previous_loss - loss_value) < tolerance:\n",
        "        patience_counter += 1\n",
        "        print(f'Early Stopping Counter: {patience_counter}/{patience}')\n",
        "        if patience_counter >= patience:\n",
        "            print('Early stopping triggered.')\n",
        "            break\n",
        "    else:\n",
        "        patience_counter = 0  # Reset counter if the loss improved\n",
        "\n",
        "    previous_loss = loss_value  # Update the loss for the next iteration\n",
        "\n",
        "# Evaluate the model's performance after training\n",
        "with torch.no_grad():  # Context manager that disables gradient calculation\n",
        "    T_pred = model(x_data)\n",
        "    mse = torch.mean((T_pred - T_data_tensor) ** 2)\n",
        "    print(f'Final Mean Squared Error: {mse.item()}')\n",
        "\n",
        "print(T_pred)\n",
        "print(T_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hTZ39448Tnw",
        "outputId": "f9a82f60-b884-4764-ff0c-128ebdcb276c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 141083.71875\n",
            "Epoch 1, Loss: 139967.765625\n",
            "Epoch 2, Loss: 138675.578125\n",
            "Epoch 3, Loss: 137368.796875\n",
            "Epoch 4, Loss: 136000.984375\n",
            "Epoch 5, Loss: 134590.734375\n",
            "Epoch 6, Loss: 133146.71875\n",
            "Epoch 7, Loss: 131681.390625\n",
            "Epoch 8, Loss: 130203.6328125\n",
            "Epoch 9, Loss: 128719.46875\n",
            "Epoch 10, Loss: 127233.0234375\n",
            "Epoch 11, Loss: 125747.2734375\n",
            "Epoch 12, Loss: 124264.5234375\n",
            "Epoch 13, Loss: 122786.453125\n",
            "Epoch 14, Loss: 121314.4296875\n",
            "Epoch 15, Loss: 119849.53125\n",
            "Epoch 16, Loss: 118392.6328125\n",
            "Epoch 17, Loss: 116944.5\n",
            "Epoch 18, Loss: 115505.6484375\n",
            "Epoch 19, Loss: 114076.6328125\n",
            "Epoch 20, Loss: 112657.84375\n",
            "Epoch 21, Loss: 111249.6484375\n",
            "Epoch 22, Loss: 109852.34375\n",
            "Epoch 23, Loss: 108466.15625\n",
            "Epoch 24, Loss: 107091.34375\n",
            "Epoch 25, Loss: 105728.0546875\n",
            "Epoch 26, Loss: 104376.46875\n",
            "Epoch 27, Loss: 103036.671875\n",
            "Epoch 28, Loss: 101708.796875\n",
            "Epoch 29, Loss: 100392.90625\n",
            "Epoch 30, Loss: 99089.078125\n",
            "Epoch 31, Loss: 97797.375\n",
            "Epoch 32, Loss: 96517.828125\n",
            "Epoch 33, Loss: 95250.4453125\n",
            "Epoch 34, Loss: 93995.2734375\n",
            "Epoch 35, Loss: 92752.28125\n",
            "Epoch 36, Loss: 91521.4921875\n",
            "Epoch 37, Loss: 90302.875\n",
            "Epoch 38, Loss: 89096.4296875\n",
            "Epoch 39, Loss: 87902.1171875\n",
            "Epoch 40, Loss: 86719.90625\n",
            "Epoch 41, Loss: 85549.7734375\n",
            "Epoch 42, Loss: 84391.65625\n",
            "Epoch 43, Loss: 83245.5078125\n",
            "Epoch 44, Loss: 82111.3203125\n",
            "Epoch 45, Loss: 80988.9921875\n",
            "Epoch 46, Loss: 79878.4921875\n",
            "Epoch 47, Loss: 78779.7578125\n",
            "Epoch 48, Loss: 77692.71875\n",
            "Epoch 49, Loss: 76617.3046875\n",
            "Epoch 50, Loss: 75553.484375\n",
            "Epoch 51, Loss: 74501.1328125\n",
            "Epoch 52, Loss: 73460.2421875\n",
            "Epoch 53, Loss: 72430.6953125\n",
            "Epoch 54, Loss: 71412.4453125\n",
            "Epoch 55, Loss: 70405.40625\n",
            "Epoch 56, Loss: 69409.5078125\n",
            "Epoch 57, Loss: 68424.671875\n",
            "Epoch 58, Loss: 67450.828125\n",
            "Epoch 59, Loss: 66487.8828125\n",
            "Epoch 60, Loss: 65535.7890625\n",
            "Epoch 61, Loss: 64594.44921875\n",
            "Epoch 62, Loss: 63663.77734375\n",
            "Epoch 63, Loss: 62743.71484375\n",
            "Epoch 64, Loss: 61834.17578125\n",
            "Epoch 65, Loss: 60935.06640625\n",
            "Epoch 66, Loss: 60046.328125\n",
            "Epoch 67, Loss: 59167.86328125\n",
            "Epoch 68, Loss: 58299.61328125\n",
            "Epoch 69, Loss: 57441.484375\n",
            "Epoch 70, Loss: 56593.3984375\n",
            "Epoch 71, Loss: 55755.27734375\n",
            "Epoch 72, Loss: 54927.03515625\n",
            "Epoch 73, Loss: 54108.609375\n",
            "Epoch 74, Loss: 53299.91015625\n",
            "Epoch 75, Loss: 52500.8515625\n",
            "Epoch 76, Loss: 51711.375\n",
            "Epoch 77, Loss: 50931.37890625\n",
            "Epoch 78, Loss: 50160.8125\n",
            "Epoch 79, Loss: 49399.5625\n",
            "Epoch 80, Loss: 48647.578125\n",
            "Epoch 81, Loss: 47904.76171875\n",
            "Epoch 82, Loss: 47171.05078125\n",
            "Epoch 83, Loss: 46446.3671875\n",
            "Epoch 84, Loss: 45730.63671875\n",
            "Epoch 85, Loss: 45023.765625\n",
            "Epoch 86, Loss: 44325.68359375\n",
            "Epoch 87, Loss: 43636.328125\n",
            "Epoch 88, Loss: 42955.59765625\n",
            "Epoch 89, Loss: 42283.45703125\n",
            "Epoch 90, Loss: 41619.78515625\n",
            "Epoch 91, Loss: 40964.52734375\n",
            "Epoch 92, Loss: 40317.609375\n",
            "Epoch 93, Loss: 39678.96484375\n",
            "Epoch 94, Loss: 39048.50390625\n",
            "Epoch 95, Loss: 38426.1640625\n",
            "Epoch 96, Loss: 37811.84765625\n",
            "Epoch 97, Loss: 37205.51171875\n",
            "Epoch 98, Loss: 36607.06640625\n",
            "Epoch 99, Loss: 36016.453125\n",
            "Epoch 100, Loss: 35433.578125\n",
            "Epoch 101, Loss: 34858.37890625\n",
            "Epoch 102, Loss: 34290.7890625\n",
            "Epoch 103, Loss: 33730.72265625\n",
            "Epoch 104, Loss: 33178.12890625\n",
            "Epoch 105, Loss: 32632.9140625\n",
            "Epoch 106, Loss: 32095.029296875\n",
            "Epoch 107, Loss: 31564.38671875\n",
            "Epoch 108, Loss: 31040.919921875\n",
            "Epoch 109, Loss: 30524.5625\n",
            "Epoch 110, Loss: 30015.248046875\n",
            "Epoch 111, Loss: 29512.900390625\n",
            "Epoch 112, Loss: 29017.455078125\n",
            "Epoch 113, Loss: 28528.837890625\n",
            "Epoch 114, Loss: 28046.986328125\n",
            "Epoch 115, Loss: 27571.8359375\n",
            "Epoch 116, Loss: 27103.298828125\n",
            "Epoch 117, Loss: 26641.33203125\n",
            "Epoch 118, Loss: 26185.8515625\n",
            "Epoch 119, Loss: 25736.806640625\n",
            "Epoch 120, Loss: 25294.1171875\n",
            "Epoch 121, Loss: 24857.720703125\n",
            "Epoch 122, Loss: 24427.556640625\n",
            "Epoch 123, Loss: 24003.55078125\n",
            "Epoch 124, Loss: 23585.6484375\n",
            "Epoch 125, Loss: 23173.775390625\n",
            "Epoch 126, Loss: 22767.873046875\n",
            "Epoch 127, Loss: 22367.869140625\n",
            "Epoch 128, Loss: 21973.716796875\n",
            "Epoch 129, Loss: 21585.3359375\n",
            "Epoch 130, Loss: 21202.669921875\n",
            "Epoch 131, Loss: 20825.65625\n",
            "Epoch 132, Loss: 20454.228515625\n",
            "Epoch 133, Loss: 20088.3359375\n",
            "Epoch 134, Loss: 19727.908203125\n",
            "Epoch 135, Loss: 19372.880859375\n",
            "Epoch 136, Loss: 19023.19921875\n",
            "Epoch 137, Loss: 18678.8046875\n",
            "Epoch 138, Loss: 18339.6328125\n",
            "Epoch 139, Loss: 18005.626953125\n",
            "Epoch 140, Loss: 17676.724609375\n",
            "Epoch 141, Loss: 17352.861328125\n",
            "Epoch 142, Loss: 17033.9921875\n",
            "Epoch 143, Loss: 16720.044921875\n",
            "Epoch 144, Loss: 16410.974609375\n",
            "Epoch 145, Loss: 16106.712890625\n",
            "Epoch 146, Loss: 15807.20703125\n",
            "Epoch 147, Loss: 15512.400390625\n",
            "Epoch 148, Loss: 15222.2412109375\n",
            "Epoch 149, Loss: 14936.662109375\n",
            "Epoch 150, Loss: 14655.6181640625\n",
            "Epoch 151, Loss: 14379.0458984375\n",
            "Epoch 152, Loss: 14106.89453125\n",
            "Epoch 153, Loss: 13839.1162109375\n",
            "Epoch 154, Loss: 13575.6416015625\n",
            "Epoch 155, Loss: 13316.4287109375\n",
            "Epoch 156, Loss: 13061.4287109375\n",
            "Epoch 157, Loss: 12810.56640625\n",
            "Epoch 158, Loss: 12563.8193359375\n",
            "Epoch 159, Loss: 12321.109375\n",
            "Epoch 160, Loss: 12082.3935546875\n",
            "Epoch 161, Loss: 11847.6240234375\n",
            "Epoch 162, Loss: 11616.751953125\n",
            "Epoch 163, Loss: 11389.7197265625\n",
            "Epoch 164, Loss: 11166.482421875\n",
            "Epoch 165, Loss: 10946.98828125\n",
            "Epoch 166, Loss: 10731.18359375\n",
            "Epoch 167, Loss: 10519.0244140625\n",
            "Epoch 168, Loss: 10310.4619140625\n",
            "Epoch 169, Loss: 10105.44921875\n",
            "Epoch 170, Loss: 9903.9365234375\n",
            "Epoch 171, Loss: 9705.87109375\n",
            "Epoch 172, Loss: 9511.22265625\n",
            "Epoch 173, Loss: 9319.91796875\n",
            "Epoch 174, Loss: 9131.93359375\n",
            "Epoch 175, Loss: 8947.22265625\n",
            "Epoch 176, Loss: 8765.728515625\n",
            "Epoch 177, Loss: 8587.4130859375\n",
            "Epoch 178, Loss: 8412.2236328125\n",
            "Epoch 179, Loss: 8240.1337890625\n",
            "Epoch 180, Loss: 8071.0810546875\n",
            "Epoch 181, Loss: 7905.037109375\n",
            "Epoch 182, Loss: 7741.94384765625\n",
            "Epoch 183, Loss: 7581.7626953125\n",
            "Epoch 184, Loss: 7424.45947265625\n",
            "Epoch 185, Loss: 7269.99853515625\n",
            "Epoch 186, Loss: 7118.31884765625\n",
            "Epoch 187, Loss: 6969.39013671875\n",
            "Epoch 188, Loss: 6823.17529296875\n",
            "Epoch 189, Loss: 6679.626953125\n",
            "Epoch 190, Loss: 6538.71142578125\n",
            "Epoch 191, Loss: 6400.38134765625\n",
            "Epoch 192, Loss: 6264.607421875\n",
            "Epoch 193, Loss: 6131.345703125\n",
            "Epoch 194, Loss: 6000.56103515625\n",
            "Epoch 195, Loss: 5872.21630859375\n",
            "Epoch 196, Loss: 5746.26708984375\n",
            "Epoch 197, Loss: 5622.68115234375\n",
            "Epoch 198, Loss: 5501.42431640625\n",
            "Epoch 199, Loss: 5382.46142578125\n",
            "Epoch 200, Loss: 5265.74755859375\n",
            "Epoch 201, Loss: 5151.2587890625\n",
            "Epoch 202, Loss: 5038.955078125\n",
            "Epoch 203, Loss: 4928.80224609375\n",
            "Epoch 204, Loss: 4820.76513671875\n",
            "Epoch 205, Loss: 4714.80908203125\n",
            "Epoch 206, Loss: 4610.90576171875\n",
            "Epoch 207, Loss: 4509.015625\n",
            "Epoch 208, Loss: 4409.111328125\n",
            "Epoch 209, Loss: 4311.15966796875\n",
            "Epoch 210, Loss: 4215.12548828125\n",
            "Epoch 211, Loss: 4120.98291015625\n",
            "Epoch 212, Loss: 4028.70166015625\n",
            "Epoch 213, Loss: 3938.2392578125\n",
            "Epoch 214, Loss: 3849.5771484375\n",
            "Epoch 215, Loss: 3762.68359375\n",
            "Epoch 216, Loss: 3677.522705078125\n",
            "Epoch 217, Loss: 3594.072021484375\n",
            "Epoch 218, Loss: 3512.302978515625\n",
            "Epoch 219, Loss: 3432.18212890625\n",
            "Epoch 220, Loss: 3353.6787109375\n",
            "Epoch 221, Loss: 3276.77880859375\n",
            "Epoch 222, Loss: 3201.440673828125\n",
            "Epoch 223, Loss: 3127.647705078125\n",
            "Epoch 224, Loss: 3055.363037109375\n",
            "Epoch 225, Loss: 2984.5703125\n",
            "Epoch 226, Loss: 2915.236572265625\n",
            "Epoch 227, Loss: 2847.33642578125\n",
            "Epoch 228, Loss: 2780.8544921875\n",
            "Epoch 229, Loss: 2715.7529296875\n",
            "Epoch 230, Loss: 2652.010009765625\n",
            "Epoch 231, Loss: 2589.604736328125\n",
            "Epoch 232, Loss: 2528.513427734375\n",
            "Epoch 233, Loss: 2468.70947265625\n",
            "Epoch 234, Loss: 2410.17236328125\n",
            "Epoch 235, Loss: 2352.879150390625\n",
            "Epoch 236, Loss: 2296.804931640625\n",
            "Epoch 237, Loss: 2241.930419921875\n",
            "Epoch 238, Loss: 2188.22998046875\n",
            "Epoch 239, Loss: 2135.685302734375\n",
            "Epoch 240, Loss: 2084.272705078125\n",
            "Epoch 241, Loss: 2033.9708251953125\n",
            "Epoch 242, Loss: 1984.7618408203125\n",
            "Epoch 243, Loss: 1936.620361328125\n",
            "Epoch 244, Loss: 1889.5313720703125\n",
            "Epoch 245, Loss: 1843.475341796875\n",
            "Epoch 246, Loss: 1798.427734375\n",
            "Epoch 247, Loss: 1754.374267578125\n",
            "Epoch 248, Loss: 1711.2913818359375\n",
            "Epoch 249, Loss: 1669.162841796875\n",
            "Epoch 250, Loss: 1627.9730224609375\n",
            "Epoch 251, Loss: 1587.6968994140625\n",
            "Epoch 252, Loss: 1548.3240966796875\n",
            "Epoch 253, Loss: 1509.8319091796875\n",
            "Epoch 254, Loss: 1472.208251953125\n",
            "Epoch 255, Loss: 1435.4295654296875\n",
            "Epoch 256, Loss: 1399.4835205078125\n",
            "Epoch 257, Loss: 1364.3516845703125\n",
            "Epoch 258, Loss: 1330.020751953125\n",
            "Epoch 259, Loss: 1296.4720458984375\n",
            "Epoch 260, Loss: 1263.6923828125\n",
            "Epoch 261, Loss: 1231.66259765625\n",
            "Epoch 262, Loss: 1200.3714599609375\n",
            "Epoch 263, Loss: 1169.8004150390625\n",
            "Epoch 264, Loss: 1139.9390869140625\n",
            "Epoch 265, Loss: 1110.7689208984375\n",
            "Epoch 266, Loss: 1082.27783203125\n",
            "Epoch 267, Loss: 1054.452392578125\n",
            "Epoch 268, Loss: 1027.27490234375\n",
            "Epoch 269, Loss: 1000.739990234375\n",
            "Epoch 270, Loss: 974.8286743164062\n",
            "Epoch 271, Loss: 949.5278930664062\n",
            "Epoch 272, Loss: 924.8270874023438\n",
            "Epoch 273, Loss: 900.710205078125\n",
            "Epoch 274, Loss: 877.1704711914062\n",
            "Epoch 275, Loss: 854.1903686523438\n",
            "Epoch 276, Loss: 831.7616577148438\n",
            "Epoch 277, Loss: 809.869384765625\n",
            "Epoch 278, Loss: 788.5038452148438\n",
            "Epoch 279, Loss: 767.6556396484375\n",
            "Epoch 280, Loss: 747.3119506835938\n",
            "Epoch 281, Loss: 727.462158203125\n",
            "Epoch 282, Loss: 708.090576171875\n",
            "Epoch 283, Loss: 689.1968383789062\n",
            "Epoch 284, Loss: 670.76416015625\n",
            "Epoch 285, Loss: 652.782470703125\n",
            "Epoch 286, Loss: 635.2435302734375\n",
            "Epoch 287, Loss: 618.1376342773438\n",
            "Epoch 288, Loss: 601.452392578125\n",
            "Epoch 289, Loss: 585.1829223632812\n",
            "Epoch 290, Loss: 569.317138671875\n",
            "Epoch 291, Loss: 553.8475952148438\n",
            "Epoch 292, Loss: 538.763916015625\n",
            "Epoch 293, Loss: 524.0576171875\n",
            "Epoch 294, Loss: 509.72149658203125\n",
            "Epoch 295, Loss: 495.7460632324219\n",
            "Epoch 296, Loss: 482.1242980957031\n",
            "Epoch 297, Loss: 468.8469543457031\n",
            "Epoch 298, Loss: 455.9061584472656\n",
            "Epoch 299, Loss: 443.2956237792969\n",
            "Epoch 300, Loss: 431.0063781738281\n",
            "Epoch 301, Loss: 419.0323181152344\n",
            "Epoch 302, Loss: 407.3637390136719\n",
            "Epoch 303, Loss: 395.9960021972656\n",
            "Epoch 304, Loss: 384.92205810546875\n",
            "Epoch 305, Loss: 374.1338195800781\n",
            "Epoch 306, Loss: 363.6258544921875\n",
            "Epoch 307, Loss: 353.3891906738281\n",
            "Epoch 308, Loss: 343.4197082519531\n",
            "Epoch 309, Loss: 333.7111511230469\n",
            "Epoch 310, Loss: 324.2560729980469\n",
            "Epoch 311, Loss: 315.04852294921875\n",
            "Epoch 312, Loss: 306.0835876464844\n",
            "Epoch 313, Loss: 297.35748291015625\n",
            "Epoch 314, Loss: 288.8593444824219\n",
            "Epoch 315, Loss: 280.58660888671875\n",
            "Epoch 316, Loss: 272.53387451171875\n",
            "Epoch 317, Loss: 264.6967468261719\n",
            "Epoch 318, Loss: 257.06787109375\n",
            "Epoch 319, Loss: 249.64317321777344\n",
            "Epoch 320, Loss: 242.41758728027344\n",
            "Epoch 321, Loss: 235.38710021972656\n",
            "Epoch 322, Loss: 228.5449676513672\n",
            "Epoch 323, Loss: 221.88925170898438\n",
            "Epoch 324, Loss: 215.4125518798828\n",
            "Epoch 325, Loss: 209.1129913330078\n",
            "Epoch 326, Loss: 202.984375\n",
            "Epoch 327, Loss: 197.02317810058594\n",
            "Epoch 328, Loss: 191.223388671875\n",
            "Epoch 329, Loss: 185.5851593017578\n",
            "Epoch 330, Loss: 180.1001434326172\n",
            "Epoch 331, Loss: 174.76759338378906\n",
            "Epoch 332, Loss: 169.58201599121094\n",
            "Epoch 333, Loss: 164.53953552246094\n",
            "Epoch 334, Loss: 159.63641357421875\n",
            "Epoch 335, Loss: 154.87054443359375\n",
            "Epoch 336, Loss: 150.23826599121094\n",
            "Epoch 337, Loss: 145.7338409423828\n",
            "Epoch 338, Loss: 141.35606384277344\n",
            "Epoch 339, Loss: 137.1022491455078\n",
            "Epoch 340, Loss: 132.96693420410156\n",
            "Epoch 341, Loss: 128.94906616210938\n",
            "Epoch 342, Loss: 125.04403686523438\n",
            "Epoch 343, Loss: 121.25015258789062\n",
            "Epoch 344, Loss: 117.56372833251953\n",
            "Epoch 345, Loss: 113.983154296875\n",
            "Epoch 346, Loss: 110.504150390625\n",
            "Epoch 347, Loss: 107.12460327148438\n",
            "Epoch 348, Loss: 103.84170532226562\n",
            "Epoch 349, Loss: 100.654052734375\n",
            "Epoch 350, Loss: 97.5577163696289\n",
            "Epoch 351, Loss: 94.55075073242188\n",
            "Epoch 352, Loss: 91.63003540039062\n",
            "Epoch 353, Loss: 88.79484558105469\n",
            "Epoch 354, Loss: 86.04219055175781\n",
            "Epoch 355, Loss: 83.369140625\n",
            "Epoch 356, Loss: 80.77507019042969\n",
            "Epoch 357, Loss: 78.25550079345703\n",
            "Epoch 358, Loss: 75.80990600585938\n",
            "Epoch 359, Loss: 73.43717956542969\n",
            "Epoch 360, Loss: 71.13361358642578\n",
            "Epoch 361, Loss: 68.8987045288086\n",
            "Epoch 362, Loss: 66.72992706298828\n",
            "Epoch 363, Loss: 64.62481689453125\n",
            "Epoch 364, Loss: 62.582462310791016\n",
            "Epoch 365, Loss: 60.600975036621094\n",
            "Epoch 366, Loss: 58.67851638793945\n",
            "Epoch 367, Loss: 56.814231872558594\n",
            "Epoch 368, Loss: 55.00543212890625\n",
            "Epoch 369, Loss: 53.251319885253906\n",
            "Epoch 370, Loss: 51.548912048339844\n",
            "Epoch 371, Loss: 49.89876937866211\n",
            "Epoch 372, Loss: 48.29841613769531\n",
            "Epoch 373, Loss: 46.746726989746094\n",
            "Epoch 374, Loss: 45.24217224121094\n",
            "Epoch 375, Loss: 43.782470703125\n",
            "Epoch 376, Loss: 42.367794036865234\n",
            "Epoch 377, Loss: 40.9967155456543\n",
            "Epoch 378, Loss: 39.66707229614258\n",
            "Epoch 379, Loss: 38.37904739379883\n",
            "Epoch 380, Loss: 37.130184173583984\n",
            "Epoch 381, Loss: 35.91957473754883\n",
            "Epoch 382, Loss: 34.746707916259766\n",
            "Epoch 383, Loss: 33.609981536865234\n",
            "Epoch 384, Loss: 32.50856018066406\n",
            "Epoch 385, Loss: 31.441619873046875\n",
            "Epoch 386, Loss: 30.407672882080078\n",
            "Epoch 387, Loss: 29.406280517578125\n",
            "Epoch 388, Loss: 28.435697555541992\n",
            "Epoch 389, Loss: 27.495847702026367\n",
            "Epoch 390, Loss: 26.58599853515625\n",
            "Epoch 391, Loss: 25.703887939453125\n",
            "Epoch 392, Loss: 24.85039520263672\n",
            "Epoch 393, Loss: 24.0233211517334\n",
            "Epoch 394, Loss: 23.22264862060547\n",
            "Epoch 395, Loss: 22.44744873046875\n",
            "Epoch 396, Loss: 21.69682502746582\n",
            "Epoch 397, Loss: 20.969919204711914\n",
            "Epoch 398, Loss: 20.26643180847168\n",
            "Epoch 399, Loss: 19.58553123474121\n",
            "Epoch 400, Loss: 18.926401138305664\n",
            "Epoch 401, Loss: 18.28852081298828\n",
            "Epoch 402, Loss: 17.671117782592773\n",
            "Epoch 403, Loss: 17.073694229125977\n",
            "Epoch 404, Loss: 16.495519638061523\n",
            "Epoch 405, Loss: 15.936120986938477\n",
            "Epoch 406, Loss: 15.394803047180176\n",
            "Epoch 407, Loss: 14.871127128601074\n",
            "Epoch 408, Loss: 14.364890098571777\n",
            "Epoch 409, Loss: 13.874517440795898\n",
            "Epoch 410, Loss: 13.400528907775879\n",
            "Epoch 411, Loss: 12.942076683044434\n",
            "Epoch 412, Loss: 12.498556137084961\n",
            "Epoch 413, Loss: 12.069815635681152\n",
            "Epoch 414, Loss: 11.655277252197266\n",
            "Epoch 415, Loss: 11.254176139831543\n",
            "Epoch 416, Loss: 10.866786003112793\n",
            "Epoch 417, Loss: 10.491768836975098\n",
            "Epoch 418, Loss: 10.12960433959961\n",
            "Epoch 419, Loss: 9.779196739196777\n",
            "Epoch 420, Loss: 9.440634727478027\n",
            "Epoch 421, Loss: 9.11343002319336\n",
            "Epoch 422, Loss: 8.797118186950684\n",
            "Epoch 423, Loss: 8.491424560546875\n",
            "Epoch 424, Loss: 8.196080207824707\n",
            "Epoch 425, Loss: 7.91048002243042\n",
            "Epoch 426, Loss: 7.634886264801025\n",
            "Epoch 427, Loss: 7.368373394012451\n",
            "Epoch 428, Loss: 7.110881328582764\n",
            "Epoch 429, Loss: 6.862493515014648\n",
            "Epoch 430, Loss: 6.622034072875977\n",
            "Epoch 431, Loss: 6.390080451965332\n",
            "Epoch 432, Loss: 6.165951728820801\n",
            "Epoch 433, Loss: 5.949598789215088\n",
            "Epoch 434, Loss: 5.7406721115112305\n",
            "Epoch 435, Loss: 5.5386962890625\n",
            "Epoch 436, Loss: 5.344053745269775\n",
            "Epoch 437, Loss: 5.155860424041748\n",
            "Epoch 438, Loss: 4.97422456741333\n",
            "Epoch 439, Loss: 4.79884147644043\n",
            "Epoch 440, Loss: 4.629548072814941\n",
            "Epoch 441, Loss: 4.466186046600342\n",
            "Epoch 442, Loss: 4.308473587036133\n",
            "Epoch 443, Loss: 4.15638542175293\n",
            "Epoch 444, Loss: 4.009409427642822\n",
            "Epoch 445, Loss: 3.8676536083221436\n",
            "Epoch 446, Loss: 3.730860710144043\n",
            "Epoch 447, Loss: 3.599013328552246\n",
            "Epoch 448, Loss: 3.471754312515259\n",
            "Epoch 449, Loss: 3.3489625453948975\n",
            "Epoch 450, Loss: 3.230520009994507\n",
            "Epoch 451, Loss: 3.1162056922912598\n",
            "Epoch 452, Loss: 3.005910634994507\n",
            "Epoch 453, Loss: 2.899629831314087\n",
            "Epoch 454, Loss: 2.7972517013549805\n",
            "Epoch 455, Loss: 2.6983768939971924\n",
            "Early Stopping Counter: 1/50\n",
            "Epoch 456, Loss: 2.603010416030884\n",
            "Early Stopping Counter: 2/50\n",
            "Epoch 457, Loss: 2.5111520290374756\n",
            "Early Stopping Counter: 3/50\n",
            "Epoch 458, Loss: 2.422520160675049\n",
            "Early Stopping Counter: 4/50\n",
            "Epoch 459, Loss: 2.3371200561523438\n",
            "Early Stopping Counter: 5/50\n",
            "Epoch 460, Loss: 2.2547738552093506\n",
            "Early Stopping Counter: 6/50\n",
            "Epoch 461, Loss: 2.175398826599121\n",
            "Early Stopping Counter: 7/50\n",
            "Epoch 462, Loss: 2.0988292694091797\n",
            "Early Stopping Counter: 8/50\n",
            "Epoch 463, Loss: 2.0251569747924805\n",
            "Early Stopping Counter: 9/50\n",
            "Epoch 464, Loss: 1.954135775566101\n",
            "Early Stopping Counter: 10/50\n",
            "Epoch 465, Loss: 1.8856128454208374\n",
            "Early Stopping Counter: 11/50\n",
            "Epoch 466, Loss: 1.8196799755096436\n",
            "Early Stopping Counter: 12/50\n",
            "Epoch 467, Loss: 1.7560311555862427\n",
            "Early Stopping Counter: 13/50\n",
            "Epoch 468, Loss: 1.6948344707489014\n",
            "Early Stopping Counter: 14/50\n",
            "Epoch 469, Loss: 1.6358696222305298\n",
            "Early Stopping Counter: 15/50\n",
            "Epoch 470, Loss: 1.5789295434951782\n",
            "Early Stopping Counter: 16/50\n",
            "Epoch 471, Loss: 1.5243194103240967\n",
            "Early Stopping Counter: 17/50\n",
            "Epoch 472, Loss: 1.4714770317077637\n",
            "Early Stopping Counter: 18/50\n",
            "Epoch 473, Loss: 1.420772671699524\n",
            "Early Stopping Counter: 19/50\n",
            "Epoch 474, Loss: 1.3718042373657227\n",
            "Early Stopping Counter: 20/50\n",
            "Epoch 475, Loss: 1.324725866317749\n",
            "Early Stopping Counter: 21/50\n",
            "Epoch 476, Loss: 1.2793523073196411\n",
            "Early Stopping Counter: 22/50\n",
            "Epoch 477, Loss: 1.2356373071670532\n",
            "Early Stopping Counter: 23/50\n",
            "Epoch 478, Loss: 1.1936594247817993\n",
            "Early Stopping Counter: 24/50\n",
            "Epoch 479, Loss: 1.1531838178634644\n",
            "Early Stopping Counter: 25/50\n",
            "Epoch 480, Loss: 1.1141695976257324\n",
            "Early Stopping Counter: 26/50\n",
            "Epoch 481, Loss: 1.0766947269439697\n",
            "Early Stopping Counter: 27/50\n",
            "Epoch 482, Loss: 1.040482997894287\n",
            "Early Stopping Counter: 28/50\n",
            "Epoch 483, Loss: 1.0058411359786987\n",
            "Early Stopping Counter: 29/50\n",
            "Epoch 484, Loss: 0.9723867177963257\n",
            "Early Stopping Counter: 30/50\n",
            "Epoch 485, Loss: 0.9401979446411133\n",
            "Early Stopping Counter: 31/50\n",
            "Epoch 486, Loss: 0.9092915058135986\n",
            "Early Stopping Counter: 32/50\n",
            "Epoch 487, Loss: 0.8794208765029907\n",
            "Early Stopping Counter: 33/50\n",
            "Epoch 488, Loss: 0.8508153557777405\n",
            "Early Stopping Counter: 34/50\n",
            "Epoch 489, Loss: 0.8231847286224365\n",
            "Early Stopping Counter: 35/50\n",
            "Epoch 490, Loss: 0.7967002987861633\n",
            "Early Stopping Counter: 36/50\n",
            "Epoch 491, Loss: 0.7711343169212341\n",
            "Early Stopping Counter: 37/50\n",
            "Epoch 492, Loss: 0.7466042041778564\n",
            "Early Stopping Counter: 38/50\n",
            "Epoch 493, Loss: 0.7229859828948975\n",
            "Early Stopping Counter: 39/50\n",
            "Epoch 494, Loss: 0.700344443321228\n",
            "Early Stopping Counter: 40/50\n",
            "Epoch 495, Loss: 0.6785163283348083\n",
            "Early Stopping Counter: 41/50\n",
            "Epoch 496, Loss: 0.6575232148170471\n",
            "Early Stopping Counter: 42/50\n",
            "Epoch 497, Loss: 0.6372984051704407\n",
            "Early Stopping Counter: 43/50\n",
            "Epoch 498, Loss: 0.6179039478302002\n",
            "Early Stopping Counter: 44/50\n",
            "Epoch 499, Loss: 0.5991920232772827\n",
            "Early Stopping Counter: 45/50\n",
            "Epoch 500, Loss: 0.5813040137290955\n",
            "Early Stopping Counter: 46/50\n",
            "Epoch 501, Loss: 0.5640568137168884\n",
            "Early Stopping Counter: 47/50\n",
            "Epoch 502, Loss: 0.547509491443634\n",
            "Early Stopping Counter: 48/50\n",
            "Epoch 503, Loss: 0.5315657258033752\n",
            "Early Stopping Counter: 49/50\n",
            "Epoch 504, Loss: 0.5162457823753357\n",
            "Early Stopping Counter: 50/50\n",
            "Early stopping triggered.\n",
            "Final Mean Squared Error: 0.5015669465065002\n",
            "tensor([[374.9993],\n",
            "        [374.9993],\n",
            "        [374.9993],\n",
            "        [374.9993],\n",
            "        [374.9993],\n",
            "        [374.9993],\n",
            "        [374.9993],\n",
            "        [374.9993],\n",
            "        [374.9993],\n",
            "        [374.9993],\n",
            "        [374.9993]])\n",
            "[376.183183, 376.079104, 375.962007, 375.814418, 375.707068, 375.600435, 375.521479, 375.386663, 375.2089, 375.126933, 374.882827]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the neural network\n",
        "class HeatConductionNN(nn.Module):\n",
        "    def __init__(self, num_neurons=20, num_layers=3):\n",
        "        super().__init__()\n",
        "        layers = [nn.Linear(1, num_neurons), nn.Tanh()]\n",
        "        for _ in range(num_layers-1):\n",
        "            layers += [nn.Linear(num_neurons, num_neurons), nn.Tanh()]\n",
        "        layers.append(nn.Linear(num_neurons, 1))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = HeatConductionNN()\n",
        "\n",
        "# Define the physics-informed loss\n",
        "def physics_informed_loss(model, x, T_actual):\n",
        "    # Use automatic differentiation to compute the second derivative\n",
        "    x.requires_grad = True\n",
        "    T_pred = model(x)\n",
        "    # dT_dx = torch.autograd.grad(T_pred, x, grad_outputs=torch.ones_like(T_pred), create_graph=True)[0]\n",
        "    # d2T_dx2 = torch.autograd.grad(dT_dx, x, grad_outputs=torch.ones_like(dT_dx), create_graph=True)[0]\n",
        "\n",
        "    # # Loss for the differential equation\n",
        "    # pde_loss = torch.mean(d2T_dx2**2)\n",
        "\n",
        "    # Loss for the data points\n",
        "    data_loss = torch.mean((T_pred - T_actual)**2)\n",
        "\n",
        "    return data_loss\n",
        "\n",
        "# Training loop\n",
        "T_data = [376.183183,\t376.079104,\t375.962007,\t375.814418,\t375.707068,\t375.600435,\t375.521479,\t375.386663,\t375.208900,\t375.126933,\t374.882827]\n",
        "\n",
        "# Convert the actual temperature data to a PyTorch tensor\n",
        "T_data_tensor = torch.tensor(T_data, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Training loop\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
        "x_data = torch.linspace(0, 0.2, 11).reshape(-1, 1)\n",
        "tolerance = 1e-1  # Set a threshold for early stopping\n",
        "previous_loss = float('inf')  # Initialize previous loss to a large value\n",
        "patience = 50  # Set how many epochs to wait after last time the monitored quantity improved\n",
        "patience_counter = 0  # Counter for patience\n",
        "\n",
        "for epoch in range(100000):\n",
        "    optimizer.zero_grad()\n",
        "    loss = physics_informed_loss(model, x_data, T_data_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_value = loss.item()\n",
        "    print(f'Epoch {epoch}, Loss: {loss_value}')\n",
        "\n",
        "    # Check for early stopping condition\n",
        "    if abs(previous_loss - loss_value) < tolerance:\n",
        "        patience_counter += 1\n",
        "        print(f'Early Stopping Counter: {patience_counter}/{patience}')\n",
        "        if patience_counter >= patience:\n",
        "            print('Early stopping triggered.')\n",
        "            break\n",
        "    else:\n",
        "        patience_counter = 0  # Reset counter if the loss improved\n",
        "\n",
        "    previous_loss = loss_value  # Update the loss for the next iteration\n",
        "\n",
        "# Evaluate the model's performance after training\n",
        "with torch.no_grad():  # Context manager that disables gradient calculation\n",
        "    T_pred = model(x_data)\n",
        "    mse = torch.mean((T_pred - T_data_tensor) ** 2)\n",
        "    print(f'Final Mean Squared Error: {mse.item()}')\n",
        "\n",
        "print(T_pred)\n",
        "print(T_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfDX_7yUkGPZ",
        "outputId": "b72d6cca-115b-43b5-b4a8-699e21a1611e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 140991.859375\n",
            "Epoch 1, Loss: 140448.484375\n",
            "Epoch 2, Loss: 138581.1875\n",
            "Epoch 3, Loss: 137282.765625\n",
            "Epoch 4, Loss: 135959.359375\n",
            "Epoch 5, Loss: 134663.109375\n",
            "Epoch 6, Loss: 133262.015625\n",
            "Epoch 7, Loss: 131807.96875\n",
            "Epoch 8, Loss: 130362.2734375\n",
            "Epoch 9, Loss: 128904.296875\n",
            "Epoch 10, Loss: 127436.15625\n",
            "Epoch 11, Loss: 125962.578125\n",
            "Epoch 12, Loss: 124487.203125\n",
            "Epoch 13, Loss: 123012.796875\n",
            "Epoch 14, Loss: 121541.5234375\n",
            "Epoch 15, Loss: 120075.046875\n",
            "Epoch 16, Loss: 118614.65625\n",
            "Epoch 17, Loss: 117161.5234375\n",
            "Epoch 18, Loss: 115716.5\n",
            "Epoch 19, Loss: 114280.3203125\n",
            "Epoch 20, Loss: 112853.59375\n",
            "Epoch 21, Loss: 111436.84375\n",
            "Epoch 22, Loss: 110030.5078125\n",
            "Epoch 23, Loss: 108634.96875\n",
            "Epoch 24, Loss: 107250.5234375\n",
            "Epoch 25, Loss: 105877.40625\n",
            "Epoch 26, Loss: 104515.8828125\n",
            "Epoch 27, Loss: 103166.1171875\n",
            "Epoch 28, Loss: 101828.25\n",
            "Epoch 29, Loss: 100502.421875\n",
            "Epoch 30, Loss: 99188.71875\n",
            "Epoch 31, Loss: 97887.2265625\n",
            "Epoch 32, Loss: 96598.0234375\n",
            "Epoch 33, Loss: 95321.1484375\n",
            "Epoch 34, Loss: 94056.625\n",
            "Epoch 35, Loss: 92804.4921875\n",
            "Epoch 36, Loss: 91564.7265625\n",
            "Epoch 37, Loss: 90337.34375\n",
            "Epoch 38, Loss: 89122.328125\n",
            "Epoch 39, Loss: 87919.65625\n",
            "Epoch 40, Loss: 86729.3203125\n",
            "Epoch 41, Loss: 85551.25\n",
            "Epoch 42, Loss: 84385.453125\n",
            "Epoch 43, Loss: 83231.8359375\n",
            "Epoch 44, Loss: 82090.3828125\n",
            "Epoch 45, Loss: 80961.0234375\n",
            "Epoch 46, Loss: 79843.7109375\n",
            "Epoch 47, Loss: 78738.3671875\n",
            "Epoch 48, Loss: 77644.9296875\n",
            "Epoch 49, Loss: 76563.3515625\n",
            "Epoch 50, Loss: 75493.5546875\n",
            "Epoch 51, Loss: 74435.4453125\n",
            "Epoch 52, Loss: 73388.9921875\n",
            "Epoch 53, Loss: 72354.0703125\n",
            "Epoch 54, Loss: 71330.65625\n",
            "Epoch 55, Loss: 70318.625\n",
            "Epoch 56, Loss: 69317.9296875\n",
            "Epoch 57, Loss: 68328.4921875\n",
            "Epoch 58, Loss: 67350.1953125\n",
            "Epoch 59, Loss: 66383.0\n",
            "Epoch 60, Loss: 65426.8125\n",
            "Epoch 61, Loss: 64481.53515625\n",
            "Epoch 62, Loss: 63547.09765625\n",
            "Epoch 63, Loss: 62623.421875\n",
            "Epoch 64, Loss: 61710.41015625\n",
            "Epoch 65, Loss: 60807.99609375\n",
            "Epoch 66, Loss: 59916.0859375\n",
            "Epoch 67, Loss: 59034.59765625\n",
            "Epoch 68, Loss: 58163.453125\n",
            "Epoch 69, Loss: 57302.56640625\n",
            "Epoch 70, Loss: 56451.8515625\n",
            "Epoch 71, Loss: 55611.234375\n",
            "Epoch 72, Loss: 54780.62890625\n",
            "Epoch 73, Loss: 53959.9375\n",
            "Epoch 74, Loss: 53149.1015625\n",
            "Epoch 75, Loss: 52348.015625\n",
            "Epoch 76, Loss: 51556.62109375\n",
            "Epoch 77, Loss: 50774.81640625\n",
            "Epoch 78, Loss: 50002.52734375\n",
            "Epoch 79, Loss: 49239.68359375\n",
            "Epoch 80, Loss: 48486.18359375\n",
            "Epoch 81, Loss: 47741.97265625\n",
            "Epoch 82, Loss: 47006.93359375\n",
            "Epoch 83, Loss: 46281.03515625\n",
            "Epoch 84, Loss: 45564.14453125\n",
            "Epoch 85, Loss: 44856.22265625\n",
            "Epoch 86, Loss: 44157.171875\n",
            "Epoch 87, Loss: 43466.9140625\n",
            "Epoch 88, Loss: 42785.390625\n",
            "Epoch 89, Loss: 42112.49609375\n",
            "Epoch 90, Loss: 41448.16015625\n",
            "Epoch 91, Loss: 40792.3125\n",
            "Epoch 92, Loss: 40144.87109375\n",
            "Epoch 93, Loss: 39505.765625\n",
            "Epoch 94, Loss: 38874.91015625\n",
            "Epoch 95, Loss: 38252.2421875\n",
            "Epoch 96, Loss: 37637.66015625\n",
            "Epoch 97, Loss: 37031.1171875\n",
            "Epoch 98, Loss: 36432.52734375\n",
            "Epoch 99, Loss: 35841.80859375\n",
            "Epoch 100, Loss: 35258.8984375\n",
            "Epoch 101, Loss: 34683.71484375\n",
            "Epoch 102, Loss: 34116.19140625\n",
            "Epoch 103, Loss: 33556.25\n",
            "Epoch 104, Loss: 33003.81640625\n",
            "Epoch 105, Loss: 32458.814453125\n",
            "Epoch 106, Loss: 31921.181640625\n",
            "Epoch 107, Loss: 31390.84375\n",
            "Epoch 108, Loss: 30867.712890625\n",
            "Epoch 109, Loss: 30351.748046875\n",
            "Epoch 110, Loss: 29842.861328125\n",
            "Epoch 111, Loss: 29340.966796875\n",
            "Epoch 112, Loss: 28846.0234375\n",
            "Epoch 113, Loss: 28357.943359375\n",
            "Epoch 114, Loss: 27876.65625\n",
            "Epoch 115, Loss: 27402.107421875\n",
            "Epoch 116, Loss: 26934.216796875\n",
            "Epoch 117, Loss: 26472.90625\n",
            "Epoch 118, Loss: 26018.130859375\n",
            "Epoch 119, Loss: 25569.8125\n",
            "Epoch 120, Loss: 25127.8828125\n",
            "Epoch 121, Loss: 24692.263671875\n",
            "Epoch 122, Loss: 24262.912109375\n",
            "Epoch 123, Loss: 23839.73828125\n",
            "Epoch 124, Loss: 23422.693359375\n",
            "Epoch 125, Loss: 23011.705078125\n",
            "Epoch 126, Loss: 22606.7109375\n",
            "Epoch 127, Loss: 22207.6328125\n",
            "Epoch 128, Loss: 21814.423828125\n",
            "Epoch 129, Loss: 21427.013671875\n",
            "Epoch 130, Loss: 21045.330078125\n",
            "Epoch 131, Loss: 20669.322265625\n",
            "Epoch 132, Loss: 20298.919921875\n",
            "Epoch 133, Loss: 19934.064453125\n",
            "Epoch 134, Loss: 19574.689453125\n",
            "Epoch 135, Loss: 19220.73828125\n",
            "Epoch 136, Loss: 18872.146484375\n",
            "Epoch 137, Loss: 18528.845703125\n",
            "Epoch 138, Loss: 18190.78125\n",
            "Epoch 139, Loss: 17857.89453125\n",
            "Epoch 140, Loss: 17530.126953125\n",
            "Epoch 141, Loss: 17207.416015625\n",
            "Epoch 142, Loss: 16889.69921875\n",
            "Epoch 143, Loss: 16576.923828125\n",
            "Epoch 144, Loss: 16269.0302734375\n",
            "Epoch 145, Loss: 15965.9541015625\n",
            "Epoch 146, Loss: 15667.640625\n",
            "Epoch 147, Loss: 15374.0400390625\n",
            "Epoch 148, Loss: 15085.0810546875\n",
            "Epoch 149, Loss: 14800.7197265625\n",
            "Epoch 150, Loss: 14520.8935546875\n",
            "Epoch 151, Loss: 14245.55078125\n",
            "Epoch 152, Loss: 13974.6337890625\n",
            "Epoch 153, Loss: 13708.0810546875\n",
            "Epoch 154, Loss: 13445.8505859375\n",
            "Epoch 155, Loss: 13187.884765625\n",
            "Epoch 156, Loss: 12934.125\n",
            "Epoch 157, Loss: 12684.5146484375\n",
            "Epoch 158, Loss: 12439.009765625\n",
            "Epoch 159, Loss: 12197.556640625\n",
            "Epoch 160, Loss: 11960.09765625\n",
            "Epoch 161, Loss: 11726.5791015625\n",
            "Epoch 162, Loss: 11496.9609375\n",
            "Epoch 163, Loss: 11271.185546875\n",
            "Epoch 164, Loss: 11049.203125\n",
            "Epoch 165, Loss: 10830.9658203125\n",
            "Epoch 166, Loss: 10616.4072265625\n",
            "Epoch 167, Loss: 10405.5087890625\n",
            "Epoch 168, Loss: 10198.19921875\n",
            "Epoch 169, Loss: 9994.427734375\n",
            "Epoch 170, Loss: 9794.162109375\n",
            "Epoch 171, Loss: 9597.33984375\n",
            "Epoch 172, Loss: 9403.9248046875\n",
            "Epoch 173, Loss: 9213.8720703125\n",
            "Epoch 174, Loss: 9027.1220703125\n",
            "Epoch 175, Loss: 8843.6396484375\n",
            "Epoch 176, Loss: 8663.365234375\n",
            "Epoch 177, Loss: 8486.2724609375\n",
            "Epoch 178, Loss: 8312.3125\n",
            "Epoch 179, Loss: 8141.4296875\n",
            "Epoch 180, Loss: 7973.58740234375\n",
            "Epoch 181, Loss: 7808.744140625\n",
            "Epoch 182, Loss: 7646.849609375\n",
            "Epoch 183, Loss: 7487.86669921875\n",
            "Epoch 184, Loss: 7331.75634765625\n",
            "Epoch 185, Loss: 7178.46826171875\n",
            "Epoch 186, Loss: 7027.96142578125\n",
            "Epoch 187, Loss: 6880.20751953125\n",
            "Epoch 188, Loss: 6735.15185546875\n",
            "Epoch 189, Loss: 6592.75634765625\n",
            "Epoch 190, Loss: 6452.990234375\n",
            "Epoch 191, Loss: 6315.802734375\n",
            "Epoch 192, Loss: 6181.16455078125\n",
            "Epoch 193, Loss: 6049.025390625\n",
            "Epoch 194, Loss: 5919.361328125\n",
            "Epoch 195, Loss: 5792.12451171875\n",
            "Epoch 196, Loss: 5667.28076171875\n",
            "Epoch 197, Loss: 5544.796875\n",
            "Epoch 198, Loss: 5424.626953125\n",
            "Epoch 199, Loss: 5306.74169921875\n",
            "Epoch 200, Loss: 5191.09716796875\n",
            "Epoch 201, Loss: 5077.67138671875\n",
            "Epoch 202, Loss: 4966.42236328125\n",
            "Epoch 203, Loss: 4857.31640625\n",
            "Epoch 204, Loss: 4750.322265625\n",
            "Epoch 205, Loss: 4645.3974609375\n",
            "Epoch 206, Loss: 4542.50439453125\n",
            "Epoch 207, Loss: 4441.62939453125\n",
            "Epoch 208, Loss: 4342.732421875\n",
            "Epoch 209, Loss: 4245.771484375\n",
            "Epoch 210, Loss: 4150.7236328125\n",
            "Epoch 211, Loss: 4057.556396484375\n",
            "Epoch 212, Loss: 3966.238037109375\n",
            "Epoch 213, Loss: 3876.730712890625\n",
            "Epoch 214, Loss: 3789.023193359375\n",
            "Epoch 215, Loss: 3703.06494140625\n",
            "Epoch 216, Loss: 3618.8349609375\n",
            "Epoch 217, Loss: 3536.3037109375\n",
            "Epoch 218, Loss: 3455.441650390625\n",
            "Epoch 219, Loss: 3376.224365234375\n",
            "Epoch 220, Loss: 3298.6162109375\n",
            "Epoch 221, Loss: 3222.59521484375\n",
            "Epoch 222, Loss: 3148.13037109375\n",
            "Epoch 223, Loss: 3075.198486328125\n",
            "Epoch 224, Loss: 3003.770263671875\n",
            "Epoch 225, Loss: 2933.822021484375\n",
            "Epoch 226, Loss: 2865.322998046875\n",
            "Epoch 227, Loss: 2798.248779296875\n",
            "Epoch 228, Loss: 2732.58203125\n",
            "Epoch 229, Loss: 2668.284423828125\n",
            "Epoch 230, Loss: 2605.34130859375\n",
            "Epoch 231, Loss: 2543.7216796875\n",
            "Epoch 232, Loss: 2483.408447265625\n",
            "Epoch 233, Loss: 2424.375\n",
            "Epoch 234, Loss: 2366.6005859375\n",
            "Epoch 235, Loss: 2310.05419921875\n",
            "Epoch 236, Loss: 2254.7236328125\n",
            "Epoch 237, Loss: 2200.575439453125\n",
            "Epoch 238, Loss: 2147.59912109375\n",
            "Epoch 239, Loss: 2095.768310546875\n",
            "Epoch 240, Loss: 2045.056640625\n",
            "Epoch 241, Loss: 1995.4512939453125\n",
            "Epoch 242, Loss: 1946.92626953125\n",
            "Epoch 243, Loss: 1899.464111328125\n",
            "Epoch 244, Loss: 1853.0421142578125\n",
            "Epoch 245, Loss: 1807.643310546875\n",
            "Epoch 246, Loss: 1763.2457275390625\n",
            "Epoch 247, Loss: 1719.83056640625\n",
            "Epoch 248, Loss: 1677.381591796875\n",
            "Epoch 249, Loss: 1635.8753662109375\n",
            "Epoch 250, Loss: 1595.2960205078125\n",
            "Epoch 251, Loss: 1555.6285400390625\n",
            "Epoch 252, Loss: 1516.8524169921875\n",
            "Epoch 253, Loss: 1478.94873046875\n",
            "Epoch 254, Loss: 1441.9044189453125\n",
            "Epoch 255, Loss: 1405.6983642578125\n",
            "Epoch 256, Loss: 1370.3165283203125\n",
            "Epoch 257, Loss: 1335.7381591796875\n",
            "Epoch 258, Loss: 1301.9539794921875\n",
            "Epoch 259, Loss: 1268.946044921875\n",
            "Epoch 260, Loss: 1236.694580078125\n",
            "Epoch 261, Loss: 1205.18701171875\n",
            "Epoch 262, Loss: 1174.4097900390625\n",
            "Epoch 263, Loss: 1144.3487548828125\n",
            "Epoch 264, Loss: 1114.9813232421875\n",
            "Epoch 265, Loss: 1086.3052978515625\n",
            "Epoch 266, Loss: 1058.2967529296875\n",
            "Epoch 267, Loss: 1030.9439697265625\n",
            "Epoch 268, Loss: 1004.239501953125\n",
            "Epoch 269, Loss: 978.1641235351562\n",
            "Epoch 270, Loss: 952.7030639648438\n",
            "Epoch 271, Loss: 927.8494262695312\n",
            "Epoch 272, Loss: 903.5863037109375\n",
            "Epoch 273, Loss: 879.9017944335938\n",
            "Epoch 274, Loss: 856.783935546875\n",
            "Epoch 275, Loss: 834.2222900390625\n",
            "Epoch 276, Loss: 812.2017822265625\n",
            "Epoch 277, Loss: 790.7142944335938\n",
            "Epoch 278, Loss: 769.745361328125\n",
            "Epoch 279, Loss: 749.2869262695312\n",
            "Epoch 280, Loss: 729.3250122070312\n",
            "Epoch 281, Loss: 709.8497314453125\n",
            "Epoch 282, Loss: 690.853759765625\n",
            "Epoch 283, Loss: 672.3218994140625\n",
            "Epoch 284, Loss: 654.245849609375\n",
            "Epoch 285, Loss: 636.6132202148438\n",
            "Epoch 286, Loss: 619.42041015625\n",
            "Epoch 287, Loss: 602.6532592773438\n",
            "Epoch 288, Loss: 586.301025390625\n",
            "Epoch 289, Loss: 570.3587646484375\n",
            "Epoch 290, Loss: 554.8131103515625\n",
            "Epoch 291, Loss: 539.6580810546875\n",
            "Epoch 292, Loss: 524.8836059570312\n",
            "Epoch 293, Loss: 510.4823913574219\n",
            "Epoch 294, Loss: 496.4433288574219\n",
            "Epoch 295, Loss: 482.7596435546875\n",
            "Epoch 296, Loss: 469.42529296875\n",
            "Epoch 297, Loss: 456.42755126953125\n",
            "Epoch 298, Loss: 443.7646789550781\n",
            "Epoch 299, Loss: 431.4245910644531\n",
            "Epoch 300, Loss: 419.40216064453125\n",
            "Epoch 301, Loss: 407.6877136230469\n",
            "Epoch 302, Loss: 396.27655029296875\n",
            "Epoch 303, Loss: 385.1591491699219\n",
            "Epoch 304, Loss: 374.3310241699219\n",
            "Epoch 305, Loss: 363.7841491699219\n",
            "Epoch 306, Loss: 353.5119323730469\n",
            "Epoch 307, Loss: 343.5079345703125\n",
            "Epoch 308, Loss: 333.7646484375\n",
            "Epoch 309, Loss: 324.2791748046875\n",
            "Epoch 310, Loss: 315.0430908203125\n",
            "Epoch 311, Loss: 306.05047607421875\n",
            "Epoch 312, Loss: 297.2943420410156\n",
            "Epoch 313, Loss: 288.772216796875\n",
            "Epoch 314, Loss: 280.4752197265625\n",
            "Epoch 315, Loss: 272.4009094238281\n",
            "Epoch 316, Loss: 264.5418701171875\n",
            "Epoch 317, Loss: 256.8927917480469\n",
            "Epoch 318, Loss: 249.45135498046875\n",
            "Epoch 319, Loss: 242.20863342285156\n",
            "Epoch 320, Loss: 235.15966796875\n",
            "Epoch 321, Loss: 228.30429077148438\n",
            "Epoch 322, Loss: 221.63304138183594\n",
            "Epoch 323, Loss: 215.14308166503906\n",
            "Epoch 324, Loss: 208.82989501953125\n",
            "Epoch 325, Loss: 202.68980407714844\n",
            "Epoch 326, Loss: 196.71754455566406\n",
            "Epoch 327, Loss: 190.90968322753906\n",
            "Epoch 328, Loss: 185.26199340820312\n",
            "Epoch 329, Loss: 179.76824951171875\n",
            "Epoch 330, Loss: 174.42660522460938\n",
            "Epoch 331, Loss: 169.23500061035156\n",
            "Epoch 332, Loss: 164.18600463867188\n",
            "Epoch 333, Loss: 159.2781982421875\n",
            "Epoch 334, Loss: 154.50709533691406\n",
            "Epoch 335, Loss: 149.869140625\n",
            "Epoch 336, Loss: 145.3614501953125\n",
            "Epoch 337, Loss: 140.98062133789062\n",
            "Epoch 338, Loss: 136.72251892089844\n",
            "Epoch 339, Loss: 132.5859832763672\n",
            "Epoch 340, Loss: 128.5662841796875\n",
            "Epoch 341, Loss: 124.66030883789062\n",
            "Epoch 342, Loss: 120.8656005859375\n",
            "Epoch 343, Loss: 117.17814636230469\n",
            "Epoch 344, Loss: 113.59664916992188\n",
            "Epoch 345, Loss: 110.11787414550781\n",
            "Epoch 346, Loss: 106.7392349243164\n",
            "Epoch 347, Loss: 103.45674133300781\n",
            "Epoch 348, Loss: 100.2701644897461\n",
            "Epoch 349, Loss: 97.17498016357422\n",
            "Epoch 350, Loss: 94.16923522949219\n",
            "Epoch 351, Loss: 91.2509765625\n",
            "Epoch 352, Loss: 88.41771697998047\n",
            "Epoch 353, Loss: 85.6675796508789\n",
            "Epoch 354, Loss: 82.99707794189453\n",
            "Epoch 355, Loss: 80.40447235107422\n",
            "Epoch 356, Loss: 77.88914489746094\n",
            "Epoch 357, Loss: 75.44668579101562\n",
            "Epoch 358, Loss: 73.07709503173828\n",
            "Epoch 359, Loss: 70.77719116210938\n",
            "Epoch 360, Loss: 68.5454330444336\n",
            "Epoch 361, Loss: 66.38028717041016\n",
            "Epoch 362, Loss: 64.2790298461914\n",
            "Epoch 363, Loss: 62.240535736083984\n",
            "Epoch 364, Loss: 60.263099670410156\n",
            "Epoch 365, Loss: 58.34468078613281\n",
            "Epoch 366, Loss: 56.4841423034668\n",
            "Epoch 367, Loss: 54.67955017089844\n",
            "Epoch 368, Loss: 52.928932189941406\n",
            "Epoch 369, Loss: 51.23150634765625\n",
            "Epoch 370, Loss: 49.58587646484375\n",
            "Epoch 371, Loss: 47.9897575378418\n",
            "Epoch 372, Loss: 46.44268035888672\n",
            "Epoch 373, Loss: 44.941856384277344\n",
            "Epoch 374, Loss: 43.487056732177734\n",
            "Epoch 375, Loss: 42.077613830566406\n",
            "Epoch 376, Loss: 40.71091079711914\n",
            "Epoch 377, Loss: 39.38596725463867\n",
            "Epoch 378, Loss: 38.10182189941406\n",
            "Epoch 379, Loss: 36.85752487182617\n",
            "Epoch 380, Loss: 35.652156829833984\n",
            "Epoch 381, Loss: 34.48336410522461\n",
            "Epoch 382, Loss: 33.351356506347656\n",
            "Epoch 383, Loss: 32.254581451416016\n",
            "Epoch 384, Loss: 31.191877365112305\n",
            "Epoch 385, Loss: 30.162761688232422\n",
            "Epoch 386, Loss: 29.165468215942383\n",
            "Epoch 387, Loss: 28.19989013671875\n",
            "Epoch 388, Loss: 27.26464080810547\n",
            "Epoch 389, Loss: 26.358680725097656\n",
            "Epoch 390, Loss: 25.481016159057617\n",
            "Epoch 391, Loss: 24.631580352783203\n",
            "Epoch 392, Loss: 23.809106826782227\n",
            "Epoch 393, Loss: 23.012357711791992\n",
            "Epoch 394, Loss: 22.24158477783203\n",
            "Epoch 395, Loss: 21.495317459106445\n",
            "Epoch 396, Loss: 20.772676467895508\n",
            "Epoch 397, Loss: 20.07337760925293\n",
            "Epoch 398, Loss: 19.39631462097168\n",
            "Epoch 399, Loss: 18.741483688354492\n",
            "Epoch 400, Loss: 18.107297897338867\n",
            "Epoch 401, Loss: 17.493776321411133\n",
            "Epoch 402, Loss: 16.90015983581543\n",
            "Epoch 403, Loss: 16.325963973999023\n",
            "Epoch 404, Loss: 15.770345687866211\n",
            "Epoch 405, Loss: 15.232498168945312\n",
            "Epoch 406, Loss: 14.712573051452637\n",
            "Epoch 407, Loss: 14.209664344787598\n",
            "Epoch 408, Loss: 13.723238945007324\n",
            "Epoch 409, Loss: 13.252778053283691\n",
            "Epoch 410, Loss: 12.797561645507812\n",
            "Epoch 411, Loss: 12.357644081115723\n",
            "Epoch 412, Loss: 11.932217597961426\n",
            "Epoch 413, Loss: 11.520715713500977\n",
            "Epoch 414, Loss: 11.122698783874512\n",
            "Epoch 415, Loss: 10.738325119018555\n",
            "Epoch 416, Loss: 10.366754531860352\n",
            "Epoch 417, Loss: 10.007379531860352\n",
            "Epoch 418, Loss: 9.659897804260254\n",
            "Epoch 419, Loss: 9.324381828308105\n",
            "Epoch 420, Loss: 8.999794960021973\n",
            "Epoch 421, Loss: 8.686405181884766\n",
            "Epoch 422, Loss: 8.383658409118652\n",
            "Epoch 423, Loss: 8.09084701538086\n",
            "Epoch 424, Loss: 7.80815315246582\n",
            "Epoch 425, Loss: 7.534895420074463\n",
            "Epoch 426, Loss: 7.270839214324951\n",
            "Epoch 427, Loss: 7.015907287597656\n",
            "Epoch 428, Loss: 6.769707202911377\n",
            "Epoch 429, Loss: 6.531858444213867\n",
            "Epoch 430, Loss: 6.3022966384887695\n",
            "Epoch 431, Loss: 6.080658435821533\n",
            "Epoch 432, Loss: 5.866297721862793\n",
            "Epoch 433, Loss: 5.659539222717285\n",
            "Epoch 434, Loss: 5.459818363189697\n",
            "Epoch 435, Loss: 5.267172813415527\n",
            "Epoch 436, Loss: 5.081068515777588\n",
            "Epoch 437, Loss: 4.901340484619141\n",
            "Epoch 438, Loss: 4.72795295715332\n",
            "Epoch 439, Loss: 4.5607380867004395\n",
            "Epoch 440, Loss: 4.39915132522583\n",
            "Epoch 441, Loss: 4.243298053741455\n",
            "Epoch 442, Loss: 4.092903137207031\n",
            "Epoch 443, Loss: 3.947821855545044\n",
            "Epoch 444, Loss: 3.8077971935272217\n",
            "Epoch 445, Loss: 3.6727559566497803\n",
            "Epoch 446, Loss: 3.542494773864746\n",
            "Epoch 447, Loss: 3.4167351722717285\n",
            "Epoch 448, Loss: 3.295408010482788\n",
            "Epoch 449, Loss: 3.1785027980804443\n",
            "Epoch 450, Loss: 3.065793037414551\n",
            "Epoch 451, Loss: 2.956960439682007\n",
            "Epoch 452, Loss: 2.852205991744995\n",
            "Epoch 453, Loss: 2.7510199546813965\n",
            "Epoch 454, Loss: 2.6535043716430664\n",
            "Early Stopping Counter: 1/50\n",
            "Epoch 455, Loss: 2.559558629989624\n",
            "Early Stopping Counter: 2/50\n",
            "Epoch 456, Loss: 2.4688053131103516\n",
            "Early Stopping Counter: 3/50\n",
            "Epoch 457, Loss: 2.3815276622772217\n",
            "Early Stopping Counter: 4/50\n",
            "Epoch 458, Loss: 2.297267436981201\n",
            "Early Stopping Counter: 5/50\n",
            "Epoch 459, Loss: 2.2161219120025635\n",
            "Early Stopping Counter: 6/50\n",
            "Epoch 460, Loss: 2.1379175186157227\n",
            "Early Stopping Counter: 7/50\n",
            "Epoch 461, Loss: 2.0624895095825195\n",
            "Early Stopping Counter: 8/50\n",
            "Epoch 462, Loss: 1.9900116920471191\n",
            "Early Stopping Counter: 9/50\n",
            "Epoch 463, Loss: 1.9198298454284668\n",
            "Early Stopping Counter: 10/50\n",
            "Epoch 464, Loss: 1.852446436882019\n",
            "Early Stopping Counter: 11/50\n",
            "Epoch 465, Loss: 1.7874667644500732\n",
            "Early Stopping Counter: 12/50\n",
            "Epoch 466, Loss: 1.7249013185501099\n",
            "Early Stopping Counter: 13/50\n",
            "Epoch 467, Loss: 1.6646831035614014\n",
            "Early Stopping Counter: 14/50\n",
            "Epoch 468, Loss: 1.6065243482589722\n",
            "Early Stopping Counter: 15/50\n",
            "Epoch 469, Loss: 1.5505894422531128\n",
            "Early Stopping Counter: 16/50\n",
            "Epoch 470, Loss: 1.4967141151428223\n",
            "Early Stopping Counter: 17/50\n",
            "Epoch 471, Loss: 1.4448564052581787\n",
            "Early Stopping Counter: 18/50\n",
            "Epoch 472, Loss: 1.3948760032653809\n",
            "Early Stopping Counter: 19/50\n",
            "Epoch 473, Loss: 1.346815824508667\n",
            "Early Stopping Counter: 20/50\n",
            "Epoch 474, Loss: 1.3004889488220215\n",
            "Early Stopping Counter: 21/50\n",
            "Epoch 475, Loss: 1.2558486461639404\n",
            "Early Stopping Counter: 22/50\n",
            "Epoch 476, Loss: 1.2129498720169067\n",
            "Early Stopping Counter: 23/50\n",
            "Epoch 477, Loss: 1.1716277599334717\n",
            "Early Stopping Counter: 24/50\n",
            "Epoch 478, Loss: 1.1318289041519165\n",
            "Early Stopping Counter: 25/50\n",
            "Epoch 479, Loss: 1.0935341119766235\n",
            "Early Stopping Counter: 26/50\n",
            "Epoch 480, Loss: 1.0566421747207642\n",
            "Early Stopping Counter: 27/50\n",
            "Epoch 481, Loss: 1.0212069749832153\n",
            "Early Stopping Counter: 28/50\n",
            "Epoch 482, Loss: 0.9871353507041931\n",
            "Early Stopping Counter: 29/50\n",
            "Epoch 483, Loss: 0.9542726278305054\n",
            "Early Stopping Counter: 30/50\n",
            "Epoch 484, Loss: 0.9226064085960388\n",
            "Early Stopping Counter: 31/50\n",
            "Epoch 485, Loss: 0.8922086954116821\n",
            "Early Stopping Counter: 32/50\n",
            "Epoch 486, Loss: 0.862989604473114\n",
            "Early Stopping Counter: 33/50\n",
            "Epoch 487, Loss: 0.8348643779754639\n",
            "Early Stopping Counter: 34/50\n",
            "Epoch 488, Loss: 0.807851254940033\n",
            "Early Stopping Counter: 35/50\n",
            "Epoch 489, Loss: 0.7818203568458557\n",
            "Early Stopping Counter: 36/50\n",
            "Epoch 490, Loss: 0.7567922472953796\n",
            "Early Stopping Counter: 37/50\n",
            "Epoch 491, Loss: 0.7326910495758057\n",
            "Early Stopping Counter: 38/50\n",
            "Epoch 492, Loss: 0.7095367908477783\n",
            "Early Stopping Counter: 39/50\n",
            "Epoch 493, Loss: 0.6872565746307373\n",
            "Early Stopping Counter: 40/50\n",
            "Epoch 494, Loss: 0.6658689975738525\n",
            "Early Stopping Counter: 41/50\n",
            "Epoch 495, Loss: 0.6453043222427368\n",
            "Early Stopping Counter: 42/50\n",
            "Epoch 496, Loss: 0.6254967451095581\n",
            "Early Stopping Counter: 43/50\n",
            "Epoch 497, Loss: 0.6065484285354614\n",
            "Early Stopping Counter: 44/50\n",
            "Epoch 498, Loss: 0.588230311870575\n",
            "Early Stopping Counter: 45/50\n",
            "Epoch 499, Loss: 0.5706731677055359\n",
            "Early Stopping Counter: 46/50\n",
            "Epoch 500, Loss: 0.5537711381912231\n",
            "Early Stopping Counter: 47/50\n",
            "Epoch 501, Loss: 0.5375861525535583\n",
            "Early Stopping Counter: 48/50\n",
            "Epoch 502, Loss: 0.5219581127166748\n",
            "Early Stopping Counter: 49/50\n",
            "Epoch 503, Loss: 0.5069814920425415\n",
            "Early Stopping Counter: 50/50\n",
            "Early stopping triggered.\n",
            "Final Mean Squared Error: 0.49260032176971436\n",
            "tensor([[375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070]])\n",
            "[376.183183, 376.079104, 375.962007, 375.814418, 375.707068, 375.600435, 375.521479, 375.386663, 375.2089, 375.126933, 374.882827]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming `df` is your DataFrame loaded with pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Load the data from CSV or other file type if needed\n",
        "df = pd.read_csv('/content/drive/MyDrive/MTP_DEC/1_para.csv')\n",
        "# Assuming the first 11 columns are the temperatures at each position\n",
        "temperatures = torch.tensor(df.iloc[:, :11].values, dtype=torch.float32)\n",
        "# Assuming the positions are fixed and known, e.g., 0, 0.02, ..., 0.2\n",
        "positions = torch.linspace(0, 0.2, steps=11)\n",
        "\n",
        "# Custom dataset to handle the pairing of positions and temperatures\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, positions, temperatures):\n",
        "        self.positions = positions\n",
        "        self.temperatures = temperatures\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.temperatures.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        position = self.positions\n",
        "        temperature = self.temperatures[idx, :]\n",
        "        return position, temperature\n",
        "\n",
        "# Instantiate the custom dataset\n",
        "dataset = CustomDataset(positions, temperatures)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = HeatConductionNN()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "RWy6ozavotKW",
        "outputId": "1e4f4fc8-38ae-4e98-b4e3-9abf250f5177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (32x11 and 1x20)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-30ee8e26e936>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpositions_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperatures_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphysics_informed_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperatures_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-30ee8e26e936>\u001b[0m in \u001b[0;36mphysics_informed_loss\u001b[0;34m(model, x_batch, T_batch)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Predict temperatures for each position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mT_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Calculate gradients for the physics-informed part of the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-30ee8e26e936>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Instantiate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x11 and 1x20)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the neural network model\n",
        "class HeatConductionNN(nn.Module):\n",
        "    def __init__(self, num_neurons=20, num_layers=3):\n",
        "        super(HeatConductionNN, self).__init__()\n",
        "        self.num_neurons = num_neurons\n",
        "        # The input layer now has 11 neurons, corresponding to 11 positions\n",
        "        layers = [nn.Linear(11, num_neurons), nn.Tanh()]\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers += [nn.Linear(num_neurons, num_neurons), nn.Tanh()]\n",
        "        # The output layer has 11 neurons, one for each position\n",
        "        layers.append(nn.Linear(num_neurons, 11))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is expected to be of shape [batch_size, num_positions]\n",
        "        return self.network(x)\n",
        "\n",
        "# Load the data from CSV\n",
        "df = pd.read_csv('/content/drive/MyDrive/MTP_DEC/1_para.csv')\n",
        "# Assuming the first 11 columns are the temperatures at each position\n",
        "temperatures = torch.tensor(df.values, dtype=torch.float32)\n",
        "# Assuming the positions are fixed and known, e.g., 0, 0.02, ..., 0.2\n",
        "positions = torch.linspace(0, 0.2, steps=11)  # Shape [11]\n",
        "\n",
        "# Custom dataset to handle the pairing of positions and temperatures\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, positions, temperatures):\n",
        "        self.positions = positions.unsqueeze(0)  # Add batch dimension\n",
        "        self.temperatures = temperatures\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.temperatures.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the same positions for each sample alongside the corresponding temperatures\n",
        "        return self.positions, self.temperatures[idx]\n",
        "\n",
        "# Instantiate the custom dataset\n",
        "dataset = CustomDataset(positions, temperatures)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Instantiate the model\n",
        "model = HeatConductionNN()\n",
        "\n",
        "# Define the physics-informed loss\n",
        "def physics_informed_loss(model, x_batch, T_batch):\n",
        "    # Ensure we can compute gradients with respect to positions\n",
        "    x_batch.requires_grad = True\n",
        "\n",
        "    # Predict temperatures for each position\n",
        "    T_pred = model(x_batch)\n",
        "\n",
        "    # Calculate gradients for the physics-informed part of the loss\n",
        "    dT_dx = torch.autograd.grad(T_pred, x_batch, grad_outputs=torch.ones_like(T_pred), create_graph=True)[0]\n",
        "    d2T_dx2 = torch.autograd.grad(dT_dx, x_batch, grad_outputs=torch.ones_like(dT_dx), create_graph=True)[0]\n",
        "\n",
        "    # Loss for the differential equation (PDE loss)\n",
        "    pde_loss = torch.mean(d2T_dx2**2)\n",
        "\n",
        "    # Loss for the data points (data loss)\n",
        "    data_loss = torch.mean((T_pred - T_batch)**2)\n",
        "\n",
        "    # Total loss is the sum of PDE loss and data loss\n",
        "    return pde_loss + data_loss\n",
        "\n",
        "# Training loop\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "tolerance = 1e-1\n",
        "previous_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(1000):  # Number of epochs may need to be adjusted\n",
        "    for positions_batch, temperatures_batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = physics_informed_loss(model, positions_batch, temperatures_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Early stopping condition\n",
        "    with torch.no_grad():\n",
        "        loss = physics_informed_loss(model, positions, temperatures)\n",
        "    loss_value = loss.item()\n",
        "    if abs(previous_loss - loss_value) < tolerance:\n",
        "        patience_counter += 1\n",
        "        if patience_counter > patience:\n",
        "            print(f'Early stopping at epoch {epoch}, loss: {loss_value}')\n",
        "            break\n",
        "    else:\n",
        "        patience_counter = 0\n",
        "    previous_loss = loss_value\n",
        "\n",
        "    if epoch % 100 == 0:  # Print the loss every 100 epochs\n",
        "        print(f'Epoch {epoch}, Loss: {loss_value}')\n",
        "\n",
        "# Evaluate the model after training\n",
        "with torch.no_grad():\n",
        "    T_pred = model(positions).reshape_as(temperatures)\n",
        "    mse = nn.functional.mse_loss(T_pred, temperatures)\n",
        "    print(f'Final MSE: {mse.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "ZNIdds-cynfg",
        "outputId": "2ead598a-7a3a-48c9-9a15-b6b6e0320dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (11) must match the size of tensor b (12) at non-singleton dimension 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-51ef8891f611>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpositions_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperatures_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphysics_informed_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperatures_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-51ef8891f611>\u001b[0m in \u001b[0;36mphysics_informed_loss\u001b[0;34m(model, x_batch, T_batch)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Loss for the data points (data loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mdata_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mT_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Total loss is the sum of PDE loss and data loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (11) must match the size of tensor b (12) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Define the neural network model\n",
        "class HeatConductionNN(nn.Module):\n",
        "    def __init__(self, num_neurons=20, num_layers=3):\n",
        "        super(HeatConductionNN, self).__init__()\n",
        "        self.num_neurons = num_neurons\n",
        "        # The input layer now has 11 neurons, corresponding to 11 positions\n",
        "        layers = [nn.Linear(11, num_neurons), nn.Tanh()]\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers += [nn.Linear(num_neurons, num_neurons), nn.Tanh()]\n",
        "        # The output layer has 11 neurons, one for each position\n",
        "        layers.append(nn.Linear(num_neurons, 11))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is expected to be of shape [batch_size, num_positions]\n",
        "        return self.network(x)\n",
        "\n",
        "# Load the data from CSV\n",
        "df = pd.read_csv('/content/drive/MyDrive/MTP_DEC/1_para.csv')\n",
        "# Assuming the first 11 columns are the temperatures at each position\n",
        "temperatures = torch.tensor(df.iloc[:, :11].values, dtype=torch.float32)\n",
        "# Assuming the positions are fixed and known, e.g., 0, 0.02, ..., 0.2\n",
        "positions = torch.linspace(0, 0.2, steps=11)  # Shape [11]\n",
        "\n",
        "# Custom dataset to handle the pairing of positions and temperatures\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, positions, temperatures):\n",
        "        self.positions = positions.unsqueeze(0)  # Add batch dimension\n",
        "        self.temperatures = temperatures\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.temperatures.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the same positions for each sample alongside the corresponding temperatures\n",
        "        return self.positions.expand(self.temperatures.size(0), -1), self.temperatures[idx]\n",
        "\n",
        "# Instantiate the custom dataset\n",
        "dataset = CustomDataset(positions, temperatures)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Instantiate the model\n",
        "model = HeatConductionNN()\n",
        "print(T_pred)\n",
        "# Define the physics-informed loss\n",
        "def physics_informed_loss(model, x_batch, T_batch):\n",
        "    # Ensure we can compute gradients with respect to positions\n",
        "    x_batch.requires_grad = True\n",
        "\n",
        "    # Predict temperatures for each position\n",
        "    T_pred = model(x_batch)\n",
        "\n",
        "    # Calculate gradients for the physics-informed part of the loss\n",
        "    dT_dx = torch.autograd.grad(T_pred, x_batch, grad_outputs=torch.ones_like(T_pred), create_graph=True)[0]\n",
        "    d2T_dx2 = torch.autograd.grad(dT_dx, x_batch, grad_outputs=torch.ones_like(dT_dx), create_graph=True)[0]\n",
        "\n",
        "    # Loss for the differential equation (PDE loss)\n",
        "    pde_loss = torch.mean(d2T_dx2**2)\n",
        "\n",
        "    # Loss for the data points (data loss)\n",
        "    data_loss = torch.mean((T_pred - T_batch)**2)\n",
        "\n",
        "    # Total loss is the sum of PDE loss and data loss\n",
        "    return pde_loss + data_loss\n",
        "\n",
        "# Training loop\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "tolerance = 1e-1\n",
        "previous_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(1000):  # Number of epochs may need to be adjusted\n",
        "    for positions_batch, temperatures_batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = physics_informed_loss(model, positions_batch, temperatures_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Early stopping condition\n",
        "    with torch.no_grad():\n",
        "        loss = physics_informed_loss(model, positions, temperatures)\n",
        "    loss_value = loss.item()\n",
        "    if abs(previous_loss - loss_value) < tolerance:\n",
        "        patience_counter += 1\n",
        "        if patience_counter > patience:\n",
        "            print(f'Early stopping at epoch {epoch}, loss: {loss_value}')\n",
        "            break\n",
        "    else:\n",
        "        patience_counter = 0\n",
        "    previous_loss = loss_value\n",
        "\n",
        "    if epoch % 100 == 0:  # Print the loss every 100 epochs\n",
        "        print(f'Epoch {epoch}, Loss: {loss_value}')\n",
        "\n",
        "# Evaluate the model after training\n",
        "with torch.no_grad():\n",
        "    T_pred = model(positions.unsqueeze(0)).squeeze()  # Remove the batch dimension\n",
        "    mse = nn.functional.mse_loss(T_pred, temperatures)\n",
        "    print(f'Final MSE: {mse.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "Pttk6ANs5dap",
        "outputId": "47024e67-b1d4-450e-88a8-4c33ec34d9d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070],\n",
            "        [375.0070]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (1000) must match the size of tensor b (32) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-51a34cf7d789>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpositions_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperatures_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphysics_informed_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperatures_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-51a34cf7d789>\u001b[0m in \u001b[0;36mphysics_informed_loss\u001b[0;34m(model, x_batch, T_batch)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Loss for the data points (data loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mdata_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mT_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Total loss is the sum of PDE loss and data loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1000) must match the size of tensor b (32) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Define the neural network model\n",
        "class HeatConductionNN(nn.Module):\n",
        "    def __init__(self, num_neurons=20, num_layers=3):\n",
        "        super(HeatConductionNN, self).__init__()\n",
        "        self.num_neurons = num_neurons\n",
        "        # The input layer now has 11 neurons, corresponding to 11 positions\n",
        "        layers = [nn.Linear(11, num_neurons), nn.Tanh()]\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers += [nn.Linear(num_neurons, num_neurons), nn.Tanh()]\n",
        "        # The output layer has 11 neurons, one for each position\n",
        "        layers.append(nn.Linear(num_neurons, 11))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is expected to be of shape [batch_size, num_positions]\n",
        "        return self.network(x)\n",
        "\n",
        "# Load the data from CSV\n",
        "df = pd.read_csv('/content/drive/MyDrive/MTP_DEC/1_para.csv')\n",
        "# Assuming the first 11 columns are the temperatures at each position\n",
        "temperatures = torch.tensor(df.iloc[:, :11].values, dtype=torch.float32)\n",
        "# Assuming the positions are fixed and known, e.g., 0, 0.02, ..., 0.2\n",
        "positions = torch.linspace(0, 0.2, steps=11)  # Shape [11]\n",
        "\n",
        "# Custom dataset to handle the pairing of positions and temperatures\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, positions, temperatures):\n",
        "        self.positions = positions.unsqueeze(0)  # Add batch dimension\n",
        "        self.temperatures = temperatures\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.temperatures.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the same positions for each sample alongside the corresponding temperatures\n",
        "        return self.positions.expand(self.temperatures.size(0), -1), self.temperatures[idx]\n",
        "\n",
        "# Instantiate the custom dataset\n",
        "dataset = CustomDataset(positions, temperatures)\n",
        "data_loader = DataLoader(dataset, batch_size=1000, shuffle=True)\n",
        "\n",
        "# Instantiate the model\n",
        "model = HeatConductionNN()\n",
        "\n",
        "# Define the physics-informed loss\n",
        "def physics_informed_loss(model, x_batch, T_batch):\n",
        "    # Ensure we can compute gradients with respect to positions\n",
        "    x_batch.requires_grad = True\n",
        "\n",
        "    # Predict temperatures for each position\n",
        "    T_pred = model(x_batch)\n",
        "\n",
        "    # Calculate gradients for the physics-informed part of the loss\n",
        "    dT_dx = torch.autograd.grad(T_pred, x_batch, grad_outputs=torch.ones_like(T_pred), create_graph=True)[0]\n",
        "    d2T_dx2 = torch.autograd.grad(dT_dx, x_batch, grad_outputs=torch.ones_like(dT_dx), create_graph=True)[0]\n",
        "\n",
        "    # Loss for the differential equation (PDE loss)\n",
        "    pde_loss = torch.mean(d2T_dx2**2)\n",
        "\n",
        "    # Loss for the data points (data loss)\n",
        "    data_loss = torch.mean((T_pred - T_batch)**2)\n",
        "\n",
        "    # Total loss is the sum of PDE loss and data loss\n",
        "    return pde_loss + data_loss\n",
        "\n",
        "# Training loop\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
        "tolerance = 1e-1\n",
        "previous_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(1000):  # Number of epochs may need to be adjusted\n",
        "    for positions_batch, temperatures_batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = physics_informed_loss(model, positions_batch, temperatures_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Early stopping condition\n",
        "    with torch.no_grad():\n",
        "        loss = physics_informed_loss(model, positions.unsqueeze(0), temperatures)\n",
        "    loss_value = loss.item()\n",
        "    if abs(previous_loss - loss_value) < tolerance:\n",
        "        patience_counter += 1\n",
        "        if patience_counter > patience:\n",
        "            print(f'Early stopping at epoch {epoch}, loss: {loss_value}')\n",
        "            break\n",
        "    else:\n",
        "        patience_counter = 0\n",
        "    previous_loss = loss_value\n",
        "\n",
        "    if epoch % 100 == 0:  # Print the loss every 100 epochs\n",
        "        print(f'Epoch {epoch}, Loss: {loss_value}')\n",
        "\n",
        "# Evaluate the model after training\n",
        "with torch.no_grad():\n",
        "    T_pred = model(positions.unsqueeze(0)).squeeze()  # Remove the batch dimension\n",
        "    mse = nn.functional.mse_loss(T_pred, temperatures)\n",
        "    print(f'Final MSE: {mse.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BUXTwoXg6LJp",
        "outputId": "0f7cdc6f-74f1-4293-b197-c2dc022a3076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8c0cf102959e>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Early stopping condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphysics_informed_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-8c0cf102959e>\u001b[0m in \u001b[0;36mphysics_informed_loss\u001b[0;34m(model, x_batch, T_batch)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Calculate gradients for the physics-informed part of the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mdT_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0md2T_dx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdT_dx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdT_dx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    395\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "db8wAhsB7Sv4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}